{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Q_yoOOoYH92"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TinyModel, self).__init__()\n",
        "\n",
        "    self.linear_1 = torch.nn.Linear(100, 200)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.linear_2 = torch.nn.Linear(200, 10)\n",
        "    self.softmax = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear_1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear_2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear_2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "  print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear_2.parameters():\n",
        "  print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBT6jJy-YluM",
        "outputId": "4de6b650-99d3-4ba8-aa63-455b957e79d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model:\n",
            "TinyModel(\n",
            "  (linear_1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear_2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Just one layer:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Model params:\n",
            "Parameter containing:\n",
            "tensor([[-0.0621, -0.0342,  0.0332,  ..., -0.0087,  0.0863, -0.0318],\n",
            "        [ 0.0561,  0.0787,  0.0772,  ..., -0.0445,  0.0465,  0.0166],\n",
            "        [ 0.0121,  0.0049, -0.0004,  ..., -0.0390, -0.0946, -0.0204],\n",
            "        ...,\n",
            "        [ 0.0729,  0.0600,  0.0724,  ...,  0.0476, -0.0671,  0.0057],\n",
            "        [-0.0367,  0.0644,  0.0784,  ..., -0.0509, -0.0932,  0.0748],\n",
            "        [-0.0961, -0.0209,  0.0741,  ...,  0.0243,  0.0146, -0.0324]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0814, -0.0954,  0.0257,  0.0423,  0.0136,  0.0816, -0.0567,  0.0857,\n",
            "         0.0109, -0.0855,  0.0264,  0.0687, -0.0996,  0.0607, -0.0070, -0.0277,\n",
            "         0.0562,  0.0955, -0.0350,  0.0633, -0.0168,  0.0463,  0.0094, -0.0466,\n",
            "         0.0040, -0.0331, -0.0264, -0.0071, -0.0005,  0.0938, -0.0657, -0.0877,\n",
            "         0.0347,  0.0915, -0.0136,  0.0733,  0.0075,  0.0635, -0.0190, -0.0170,\n",
            "        -0.0226, -0.0105, -0.0870, -0.0506,  0.0412,  0.0196, -0.0887, -0.0699,\n",
            "        -0.0106,  0.0345, -0.0370, -0.0472,  0.0775,  0.0493, -0.0149,  0.0807,\n",
            "         0.0350, -0.0318, -0.0310, -0.0605,  0.0088,  0.0708, -0.0890, -0.0887,\n",
            "         0.0218,  0.0702, -0.0876,  0.0967,  0.0518,  0.0088, -0.0757,  0.0596,\n",
            "         0.0663,  0.0058, -0.0047, -0.0223, -0.0869, -0.0816, -0.0697, -0.0994,\n",
            "         0.0654, -0.0208,  0.0268, -0.0689,  0.0349, -0.0683,  0.0330, -0.0876,\n",
            "         0.0169, -0.0263, -0.0058, -0.0066, -0.0358,  0.0970, -0.0098,  0.0792,\n",
            "         0.0908,  0.0029, -0.0521,  0.0461,  0.0504,  0.0759, -0.0407,  0.0407,\n",
            "         0.0294, -0.0602,  0.0462, -0.0693, -0.0705, -0.0072,  0.0681,  0.0211,\n",
            "         0.0186, -0.0422, -0.0456, -0.0633,  0.0706, -0.0481, -0.0139,  0.0560,\n",
            "         0.0945,  0.0196,  0.0446, -0.0613,  0.0226, -0.0721, -0.0232,  0.0512,\n",
            "         0.0780,  0.0943, -0.0054,  0.0846, -0.0365,  0.0523,  0.0994,  0.0608,\n",
            "         0.0493,  0.0341, -0.0393, -0.0212,  0.0054, -0.0352, -0.0836, -0.0135,\n",
            "        -0.0499,  0.0969, -0.0728, -0.0814,  0.1000,  0.0582, -0.0868, -0.0852,\n",
            "         0.0012,  0.0257,  0.0650, -0.0490, -0.0639,  0.0455, -0.0581,  0.0776,\n",
            "        -0.0691, -0.0165,  0.0666, -0.0453,  0.0630,  0.0452, -0.0611,  0.0639,\n",
            "         0.0705,  0.0524, -0.0255,  0.0650,  0.0844,  0.0657, -0.0977,  0.0775,\n",
            "        -0.0927,  0.0383,  0.0919,  0.0022, -0.0979, -0.0791,  0.0383,  0.0452,\n",
            "         0.0406,  0.0238, -0.0768, -0.0573, -0.0093, -0.0237,  0.0821,  0.0339,\n",
            "         0.0759,  0.0243, -0.0190, -0.0552, -0.0828, -0.0606, -0.0030, -0.0086],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0658, -0.0248,  0.0600,  ..., -0.0519,  0.0589, -0.0527],\n",
            "        [ 0.0494, -0.0590,  0.0278,  ..., -0.0244,  0.0119, -0.0466],\n",
            "        [ 0.0296,  0.0302, -0.0342,  ..., -0.0522, -0.0050,  0.0510],\n",
            "        ...,\n",
            "        [-0.0535,  0.0343,  0.0112,  ...,  0.0641,  0.0540,  0.0266],\n",
            "        [-0.0022,  0.0147,  0.0377,  ...,  0.0346, -0.0283, -0.0141],\n",
            "        [-0.0497, -0.0286, -0.0603,  ..., -0.0037, -0.0251, -0.0095]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0563, -0.0500,  0.0196, -0.0678, -0.0611, -0.0488,  0.0573,  0.0361,\n",
            "        -0.0527, -0.0309], requires_grad=True)\n",
            "\n",
            "\n",
            "Layer params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0658, -0.0248,  0.0600,  ..., -0.0519,  0.0589, -0.0527],\n",
            "        [ 0.0494, -0.0590,  0.0278,  ..., -0.0244,  0.0119, -0.0466],\n",
            "        [ 0.0296,  0.0302, -0.0342,  ..., -0.0522, -0.0050,  0.0510],\n",
            "        ...,\n",
            "        [-0.0535,  0.0343,  0.0112,  ...,  0.0641,  0.0540,  0.0266],\n",
            "        [-0.0022,  0.0147,  0.0377,  ...,  0.0346, -0.0283, -0.0141],\n",
            "        [-0.0497, -0.0286, -0.0603,  ..., -0.0037, -0.0251, -0.0095]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0563, -0.0500,  0.0196, -0.0678, -0.0611, -0.0488,  0.0573,  0.0361,\n",
            "        -0.0527, -0.0309], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "  print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuCtWgRcDIF",
        "outputId": "5e0cecf3-9ee5-4e13-984d-c6cf2c1d6da9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([[0.1586, 0.0267, 0.2324]])\n",
            "\n",
            "\n",
            "Weight and Bias parameters:\n",
            "Parameter containing:\n",
            "tensor([[ 0.3758, -0.0345,  0.1256],\n",
            "        [-0.5111, -0.0332, -0.5457]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.3554,  0.4811], requires_grad=True)\n",
            "\n",
            "\n",
            "Output:\n",
            "tensor([[-0.2675,  0.2723]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layers"
      ],
      "metadata": {
        "id": "sGR_E19oLJd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.functional as F"
      ],
      "metadata": {
        "id": "vajVnmkSLMtB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.conv_1 = torch.nn.Conv2d(1, 6, 5)\n",
        "    self.conv_2 = torch.nn.Conv2d(6, 16, 3)\n",
        "\n",
        "    self.fc_1 = torch.nn.Linear(16*6*6, 120)\n",
        "    self.fc_2 = torch.nn.Linear(120, 84)\n",
        "    self.fc_3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv_1(x)), (2, 2))\n",
        "    x = F.max_pool2d(F.relu(self.conv_2(x)), 2)\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    x = F.relu(self.fc_1(x))\n",
        "    x = F.relu(self.fc_2(x))\n",
        "    x = self.fc_3(x)\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:]\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features"
      ],
      "metadata": {
        "id": "AAFhQUp5MKkM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent Layers"
      ],
      "metadata": {
        "id": "Yuv5QypIPBmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "vocab_size: number of words in the input vocabulary.\n",
        "tagset_size: number of tags in the output set.\n",
        "embedding_dim: size of embedding space for the vocabulary.\n",
        "hidden_dim: size of LSTM's memory\n",
        "\"\"\"\n",
        "\n",
        "class LSTMTagger(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "    # with dimensionality hidden_dim\n",
        "    self.lstm = torch.nn.LSTM(embedding_dim, tagset_size)\n",
        "\n",
        "    # Linear layer that maps from hidden state space to tag space\n",
        "    self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "metadata": {
        "id": "yBxTbDhjg3Rz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Manipulation Layers"
      ],
      "metadata": {
        "id": "x-XLx7s3lRcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 6, 6)\n",
        "print(my_tensor)\n",
        "\n",
        "maxpool_layer = torch.nn.MaxPool2d(3)\n",
        "print(maxpool_layer(my_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90q81H7hwGEh",
        "outputId": "9d6aae3b-ffa4-45cc-e6e0-5788ad6d3c07"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.6559, 0.9109, 0.2696, 0.6445, 0.4191, 0.6911],\n",
            "         [0.2124, 0.0079, 0.2259, 0.2805, 0.7323, 0.7055],\n",
            "         [0.2805, 0.7007, 0.4073, 0.8428, 0.3991, 0.9625],\n",
            "         [0.4840, 0.5453, 0.7686, 0.7415, 0.5565, 0.5647],\n",
            "         [0.1706, 0.4429, 0.6901, 0.0588, 0.4640, 0.0817],\n",
            "         [0.5249, 0.5892, 0.5700, 0.6906, 0.5943, 0.9279]]])\n",
            "tensor([[[0.9109, 0.9625],\n",
            "         [0.7686, 0.9279]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
        "print(my_tensor)\n",
        "\n",
        "print(my_tensor.mean())\n",
        "\n",
        "norm_layer = torch.nn.BatchNorm1d(4)\n",
        "normed_tensor = norm_layer(my_tensor)\n",
        "print(normed_tensor)\n",
        "\n",
        "print(normed_tensor.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33LWrmWTwaEp",
        "outputId": "7c693153-c1fa-451b-c875-6297760603c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 9.2384, 22.8886,  5.0006, 17.7150],\n",
            "         [12.5224, 23.3427, 23.9775, 17.5678],\n",
            "         [10.1309, 13.7285, 13.8641,  9.8258],\n",
            "         [ 8.4485, 14.6576, 23.3204, 14.7525]]])\n",
            "tensor(15.0613)\n",
            "tensor([[[-0.6387,  1.3107, -1.2439,  0.5719],\n",
            "         [-1.4633,  0.8548,  0.9908, -0.3824],\n",
            "         [-0.9183,  0.9626,  1.0336, -1.0778],\n",
            "         [-1.2939, -0.1204,  1.5168, -0.1025]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "tensor(-4.2841e-08, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 4, 4)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.4)\n",
        "print(dropout(my_tensor))\n",
        "print(dropout(my_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuRXaiUEfvJ-",
        "outputId": "461db434-c5fe-4105-cb79-070de134847d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.8828, 0.1952, 0.8119, 0.0000],\n",
            "         [0.0000, 0.3553, 0.7614, 0.0000],\n",
            "         [0.0657, 0.0000, 0.6914, 0.0000],\n",
            "         [1.4877, 1.1364, 0.0000, 0.0000]]])\n",
            "tensor([[[0.8828, 0.1952, 0.0000, 1.0207],\n",
            "         [0.6426, 0.3553, 0.7614, 0.0000],\n",
            "         [0.0657, 0.1556, 0.6914, 0.2428],\n",
            "         [1.4877, 0.0000, 1.6366, 1.1878]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5hdnwaWhKz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}